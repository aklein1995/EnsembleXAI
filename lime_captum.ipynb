{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://captum.ai/tutorials/Image_and_Text_Classification_LIME\n",
    "This tutorial focuses on showing how to use Captum's implementation of Local Interpretable Model-agnostic Explanations (LIME) to understand neural models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from captum.attr import visualization as viz\n",
    "from captum.attr import Lime, LimeBase\n",
    "from captum._utils.models.linear_model import SkLearnLinearRegression, SkLearnLasso\n",
    "\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import resnet18\n",
    "from torchvision.datasets import VOCSegmentation\n",
    "import torchvision.transforms as T\n",
    "from captum.attr._core.lime import get_exp_kernel_similarity_function\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load the model and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet = resnet18(pretrained=True)\n",
    "resnet = resnet.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### (train data) ImageNet 1k: mapping of label index and text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -P $HOME/.torch/models https://s3.amazonaws.com/deep-learning-models/image-models/imagenet_class_index.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_path = os.getenv('HOME') + '/.torch/models/imagenet_class_index.json'\n",
    "with open(labels_path) as json_data:\n",
    "    idx_to_labels = {idx: label for idx, [_, label] in json.load(json_data).items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (test data) PASCAL VOC 2012 as the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voc_ds = VOCSegmentation(\n",
    "    './VOC',\n",
    "    year='2012',\n",
    "    image_set='train',\n",
    "    download=True, #change to true if not downloaded\n",
    "    transform=T.Compose([\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406],\n",
    "            std=[0.229, 0.224, 0.225]\n",
    "        )    \n",
    "    ]),\n",
    "    target_transform=T.Lambda(\n",
    "        lambda p: torch.tensor(p.getdata()).view(1, p.size[1], p.size[0])\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's pick one example to see how the image and corresponding mask look like. Here we choose an image with more than one segments besides background so that we can compare each segment's impact on the classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_idx = 439\n",
    "\n",
    "def show_image(ind): \n",
    "    fig, ax = plt.subplots(1, 2, figsize=[6.4 * 2, 4.8])\n",
    "    for i, (name, source) in enumerate(zip(['Image', 'Mask'], [voc_ds.images, voc_ds.masks])):\n",
    "        ax[i].imshow(Image.open(source[ind]));\n",
    "        ax[i].set_title(f\"{name} {ind}\")\n",
    "        ax[i].axis('off')\n",
    "\n",
    "show_image(sample_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Baseline classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check how well our model works with the above example. The original Resnet only gives the logits of labels, so we will add a softmax layer to normalize them into probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, seg_mask = voc_ds[sample_idx]  # tensors of shape (channel, hight, width)\n",
    "print('Img shape:',img.shape)\n",
    "print('Seg shape:',seg_mask.shape)\n",
    "\n",
    "\n",
    "outputs = resnet(img.unsqueeze(0))\n",
    "output_probs = F.softmax(outputs, dim=1).squeeze(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "present the top 5 predicted labels to verify the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_result(probs, topk=1):\n",
    "    probs, label_indices = torch.topk(probs, topk)\n",
    "    probs = probs.tolist()\n",
    "    label_indices = label_indices.tolist()\n",
    "    for prob, idx in zip(probs, label_indices):\n",
    "        label = idx_to_labels[str(idx)]\n",
    "        print(f'{label} ({idx}):', round(prob, 4))\n",
    "        \n",
    "print_result(output_probs, topk=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Inspect the model prediction with Lime\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Generate masks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case of an image input, the feature mask is a 2D image of the same size, where each pixel in the mask indicates the feature group it belongs to via an integer value. Pixels of the same value define a group.\n",
    "\n",
    "This means we can readily use VOC's segmentation masks as feature masks for Captum!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_ids = sorted(seg_mask.unique().tolist())\n",
    "print('Segmentation IDs:', seg_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, while segmentaion numbers range from 0 to 255, Captum prefers consecutive group IDs for efficiency. Therefore, we will also include extra steps to convert mask IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map segment IDs to feature group IDs\n",
    "feature_mask = seg_mask.clone()\n",
    "for i, seg_id in enumerate(seg_ids):\n",
    "    feature_mask[feature_mask == seg_id] = i\n",
    "    \n",
    "print('Feature mask:', feature_mask)\n",
    "print('Feature mask IDs:', feature_mask.unique().tolist())\n",
    "print('Feature mask shape:',feature_mask.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Lime: train an interpretrable surrogate model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarity function. \n",
    "Because Lime aims to explain the local behavior of an example, it will reweight the training samples according to their similarity distances. By default, Captum's Lime uses the exponential kernel on top of the consine distance. We will change to euclidean distance instead which is more popular in vision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_eucl_distance = get_exp_kernel_similarity_function('euclidean', kernel_width=1000)\n",
    "\n",
    "lr_lime = Lime(\n",
    "    resnet, \n",
    "    interpretable_model=SkLearnLinearRegression(),  # build-in wrapped sklearn Linear Regression\n",
    "    similarity_func=exp_eucl_distance\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will analyze these groups' influence on the most confident prediction television. Every time we call Lime's attribute function, an interpretable model is trained around the given input, so unlike many other Captum's attribution algorithms, it is strongly recommended to only provide a single example as input (tensors with first dimension or batch size = 1).\n",
    "\n",
    "Setting the `perturbations_per_eval` can batch multiple samples in one forward pass to shorten the process as long as your machine still has capacity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_idx = output_probs.argmax().unsqueeze(0)\n",
    "\n",
    "attrs = lr_lime.attribute(\n",
    "    img.unsqueeze(0),\n",
    "    target=label_idx,\n",
    "    feature_mask=feature_mask.unsqueeze(0),\n",
    "    n_samples=40,\n",
    "    perturbations_per_eval=16,\n",
    "    show_progress=True\n",
    ").squeeze(0)\n",
    "\n",
    "print('Label idx (should be tv):',label_idx)\n",
    "print('Attribution range:', attrs.min().item(), 'to', attrs.max().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Visualize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_attr(attr_map):\n",
    "    viz.visualize_image_attr(\n",
    "        attr_map.permute(1, 2, 0).numpy(),  # adjust shape to height, width, channels \n",
    "        method='heat_map',\n",
    "        sign='all',\n",
    "        show_colorbar=True\n",
    "    )\n",
    "    \n",
    "show_attr(attrs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let's do the same, but generating other segments with skimage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.segmentation import slic\n",
    "\n",
    "segmenter=slic(image=img.permute(1, 2, 0) ,n_segments=100)\n",
    "seg_mask=torch.tensor(segmenter)\n",
    "\n",
    "seg_ids = sorted(seg_mask.unique().tolist())\n",
    "print('Segmentation IDs:', seg_ids)\n",
    "print('Segmentation shape:', seg_mask.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the created segments "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.segmentation import mark_boundaries\n",
    "img_wboundaries = mark_boundaries(img.permute(1,2,0).numpy(), segmenter)\n",
    "plt.imshow(img_wboundaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map segment IDs to feature group IDs\n",
    "feature_mask = seg_mask.clone()\n",
    "for i, seg_id in enumerate(seg_ids):\n",
    "    feature_mask[feature_mask == seg_id] = i\n",
    "    \n",
    "print('Feature mask:', feature_mask)\n",
    "print('Feature mask IDs:', feature_mask.unique().tolist())\n",
    "print('Feature mask shape:',feature_mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_idx = output_probs.argmax().unsqueeze(0)\n",
    "\n",
    "attrs = lr_lime.attribute(\n",
    "    img.unsqueeze(0),\n",
    "    target=label_idx,\n",
    "    feature_mask=feature_mask.unsqueeze(0),\n",
    "    n_samples=1000,\n",
    "    perturbations_per_eval=64,\n",
    "    show_progress=True\n",
    ").squeeze(0)\n",
    "\n",
    "print('Label idx (should be tv):',label_idx)\n",
    "print('Attribution range:', attrs.min().item(), 'to', attrs.max().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "viz.visualize_image_attr(\n",
    "    attrs.permute(1, 2, 0).numpy(),  # adjust shape to height, width, channels \n",
    "    method='heat_map',\n",
    "    sign='all',\n",
    "    show_colorbar=True\n",
    ")\n",
    "\n",
    "viz.visualize_image_attr(\n",
    "    attrs.permute(1, 2, 0).numpy(),  # adjust shape to height, width, channels \n",
    "    method='blended_heat_map',\n",
    "    sign='all',\n",
    "    show_colorbar=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
